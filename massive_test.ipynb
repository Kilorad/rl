{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "from gym_aa import AA_gun_simple0_env\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#массовый тест моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверь на зенитке, на cartpole и на mountain car\n",
    "env_class = AA_gun_simple0_env\n",
    "\n",
    "score_dict_full={'random':[],'DQN':[],'goal':[],'sarsa':[]}\n",
    "border_med = 100\n",
    "EPISODES=250\n",
    "score_dict_med={'random':[],'DQN':[],'goal':[],'sarsa':[]}#c 100 по... такты. Надо, чтобы проверить быстроту обучения\n",
    "\n",
    "agent_list=[SarsaAgent,DoubleDQNAgent,randomAgent,GoalAgent]\n",
    "for ag_num in range(4):\n",
    "    if ag_num==0:\n",
    "        name='sarsa'\n",
    "    if ag_num==1:\n",
    "        name='DQN'    \n",
    "    if ag_num==2:\n",
    "        name='random'\n",
    "    if ag_num==3:\n",
    "        name='goal'\n",
    "    for estimation in range(5):\n",
    "        print('_____',name,estimation)\n",
    "        #здесь весь код от инициализации модели до выдачи scores. Но без рендера.\n",
    "        # In case of CartPole-v1, maximum length of episode is 500\n",
    "        \n",
    "        env = env_class()\n",
    "        #env = gym.make('CartPole-v1')\n",
    "        #env=CartPoleEnv9()\n",
    "        # get size of state and action from environment\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        #agent = DoubleDQNAgent(state_size, action_size)\n",
    "        agent = agent_list[ag_num](state_size, action_size)\n",
    "        agent.render=True\n",
    "\n",
    "        scores, episodes = [], []\n",
    "        reward_lst = []\n",
    "        s_list=[]\n",
    "        a_list=[]\n",
    "\n",
    "        for e in range(EPISODES):\n",
    "            done = False\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "\n",
    "            while not done:\n",
    "                #if (e in range(2,7)) or (e in range(20,25)) or (e in range(100,103)) or (e in range(200,202)) or (e in range(300,306)) or (e in range(400,406)) or (e in range(500,506)) or (e in range(600,604)):\n",
    "                #    if agent.render:\n",
    "                #        env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                # if an action make the episode end, then gives penalty of -100\n",
    "\n",
    "\n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                #if next_state[0,11]!=reward:\n",
    "                #    print('state[13]!=reward',state[0,11],reward)\n",
    "                #\n",
    "                s_list.append(state)\n",
    "                a_list.append(action)\n",
    "                reward_lst.append(reward)\n",
    "                #\n",
    "\n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    # every episode update the target model to be same with model\n",
    "                    agent.update_target_model()\n",
    "\n",
    "                    # every episode, plot the play time\n",
    "                    scores.append(score)\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, scores, 'b')\n",
    "                    #pylab.savefig(\"./save_graph/aa_gun_dqn.png\")\n",
    "                    print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                          len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                    # if the mean of scores of last 10 episode is bigger than 490\n",
    "                    # stop training\n",
    "\n",
    "            # save the model\n",
    "            #if e % 50 == 0:\n",
    "            #    agent.model.save_weights(\"./save_model/aa_gun_dqn.h5\")\n",
    "\n",
    "\n",
    "\n",
    "        #и первые 3000 тактов - это рандом\n",
    "        #Ходов так 50\n",
    "        score_dict_full[name].append(np.mean(scores))\n",
    "        score_dict_med[name].append(np.mean(scores[border_med:]))\n",
    "        import pickle\n",
    "        f=open('score_dict.pkl','wb')\n",
    "        pickle.dump([score_dict_full,score_dict_med],f)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
