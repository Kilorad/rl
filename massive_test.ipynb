{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './env')\n",
    "sys.path.insert(1, './agents')\n",
    "\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import strategy_imitation, sarsa, ddqn, random_agent, a2c\n",
    "import aa_gun\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#массовый тест моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ random 0\n",
      "hit\n",
      "episode: 0   score: 1.9375   memory length: 0   epsilon: 1\n",
      "hit\n",
      "hit\n",
      "episode: 1   score: 3.3125   memory length: 0   epsilon: 1\n",
      "episode: 2   score: 0   memory length: 0   epsilon: 1\n",
      "episode: 3   score: 0   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 4   score: 1.4375   memory length: 0   epsilon: 1\n",
      "episode: 5   score: 0   memory length: 0   epsilon: 1\n",
      "episode: 6   score: 0.5625   memory length: 0   epsilon: 1\n",
      "episode: 7   score: 0.25   memory length: 0   epsilon: 1\n",
      "episode: 8   score: 0.75   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 9   score: 1.5   memory length: 0   epsilon: 1\n",
      "episode: 10   score: 0.5625   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 11   score: 2.125   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 12   score: 1.8125   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 13   score: 1.8125   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 14   score: 1.3125   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 15   score: 2.25   memory length: 0   epsilon: 1\n",
      "hit\n",
      "hit\n",
      "episode: 16   score: 2.75   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 17   score: 1.5625   memory length: 0   epsilon: 1\n",
      "episode: 18   score: 0.5625   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 19   score: 1.375   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 20   score: 1.1875   memory length: 0   epsilon: 1\n",
      "episode: 21   score: 0.8125   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 22   score: 1.5   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 23   score: 1.5625   memory length: 0   epsilon: 1\n",
      "episode: 24   score: 0.6875   memory length: 0   epsilon: 1\n",
      "episode: 25   score: 1.125   memory length: 0   epsilon: 1\n",
      "episode: 26   score: 1.1875   memory length: 0   epsilon: 1\n",
      "episode: 27   score: 1.125   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 28   score: 2.125   memory length: 0   epsilon: 1\n",
      "episode: 29   score: 0.5625   memory length: 0   epsilon: 1\n",
      "episode: 30   score: 0.5625   memory length: 0   epsilon: 1\n",
      "episode: 31   score: 0.75   memory length: 0   epsilon: 1\n",
      "hit\n",
      "hit\n",
      "episode: 32   score: 2.6875   memory length: 0   epsilon: 1\n",
      "episode: 33   score: 0.5625   memory length: 0   epsilon: 1\n",
      "episode: 34   score: 0.4375   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 35   score: 1.4375   memory length: 0   epsilon: 1\n",
      "episode: 36   score: 0.5625   memory length: 0   epsilon: 1\n",
      "episode: 37   score: 1.375   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 38   score: 2.0625   memory length: 0   epsilon: 1\n",
      "episode: 39   score: 0.6875   memory length: 0   epsilon: 1\n",
      "hit\n",
      "hit\n",
      "episode: 40   score: 2.625   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 41   score: 2.125   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 42   score: 1.1875   memory length: 0   epsilon: 1\n",
      "hit\n",
      "episode: 43   score: 1.625   memory length: 0   epsilon: 1\n",
      "episode: 44   score: 0.3125   memory length: 0   epsilon: 1\n",
      "episode: 45   score: 1.125   memory length: 0   epsilon: 1\n"
     ]
    }
   ],
   "source": [
    "#Проверь на зенитке, на cartpole и на mountain car\n",
    "\n",
    "score_dict_full={'random':[],'ddqn':[],'a2c':[],'sarsa':[]}\n",
    "border_med = 100\n",
    "EPISODES=140\n",
    "score_dict_med={'random':[],'ddqn':[],'a2c':[],'sarsa':[]}#c border_med по... такты. Надо, чтобы проверить быстроту обучения\n",
    "\n",
    "agent_list=[random_agent.randomAgent,a2c.A2CAgent,ddqn.DoubleDQNAgent,sarsa.SarsaAgent]\n",
    "for ag_num in range(4):\n",
    "    if ag_num==0:\n",
    "        name='random'\n",
    "    if ag_num==1:\n",
    "        name='a2c'\n",
    "    if ag_num==2:\n",
    "        name='ddqn' \n",
    "    if ag_num==3:\n",
    "        name='sarsa'\n",
    "    \n",
    "    for estimation in range(5):\n",
    "        print('_____',name,estimation)\n",
    "        #здесь весь код от инициализации модели до выдачи scores. Но без рендера.\n",
    "        # In case of CartPole-v1, maximum length of episode is 500\n",
    "        \n",
    "        env = aa_gun.AA_gun_simple0_env()\n",
    "        #env = gym.make('MountainCar-v0')\n",
    "        \n",
    "        #env=CartPoleEnv9()\n",
    "        # get size of state and action from environment\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        #agent = DoubleDQNAgent(state_size, action_size)\n",
    "        agent = agent_list[ag_num](state_size, action_size)\n",
    "        agent.train_start=100\n",
    "        agent.render=True\n",
    "\n",
    "        scores, episodes = [], []\n",
    "        reward_lst = []\n",
    "        s_list=[]\n",
    "        a_list=[]\n",
    "\n",
    "        for e in range(EPISODES):\n",
    "            done = False\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "\n",
    "            while not done:\n",
    "                #if (e in range(2,7)) or (e in range(20,25)) or (e in range(100,103)) or (e in range(200,202)) or (e in range(300,306)) or (e in range(400,406)) or (e in range(500,506)) or (e in range(600,604)):\n",
    "                #    if agent.render:\n",
    "                #        env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                # if an action make the episode end, then gives penalty of -100\n",
    "\n",
    "                \n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                #if next_state[0,11]!=reward:\n",
    "                #    print('state[13]!=reward',state[0,11],reward)\n",
    "                #\n",
    "                s_list.append(state)\n",
    "                a_list.append(action)\n",
    "                reward_lst.append(reward)\n",
    "                #\n",
    "\n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    # every episode update the target model to be same with model\n",
    "                    agent.update_target_model()\n",
    "\n",
    "                    # every episode, plot the play time\n",
    "                    scores.append(score)\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, scores, 'b')\n",
    "                    #pylab.savefig(\"./save_graph/aa_gun_dqn.png\")\n",
    "                    print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                          len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "            # save the model\n",
    "            #if e % 50 == 0:\n",
    "            #    agent.model.save_weights(\"./save_model/aa_gun_dqn.h5\")\n",
    "\n",
    "\n",
    "\n",
    "        #и первые 3000 тактов - это рандом\n",
    "        #Ходов так 50\n",
    "        score_dict_full[name].append(np.mean(scores))\n",
    "        score_dict_med[name].append(np.mean(scores[border_med:]))\n",
    "        import pickle\n",
    "        f=open('score_dict.pkl','wb')\n",
    "        pickle.dump([score_dict_full,score_dict_med],f)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
