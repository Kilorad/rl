{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gazebo_circuit2_turtlebot_lidar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c38a1dc14987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstrategy_imitation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msarsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maa_gun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgazebo_circuit2_turtlebot_lidar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gazebo_circuit2_turtlebot_lidar'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './env')\n",
    "sys.path.insert(1, './agents')\n",
    "\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import strategy_imitation, sarsa, ddqn, random_agent, a2c\n",
    "import aa_gun\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#массовый тест моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверь на зенитке, на cartpole и на mountain car\n",
    "\n",
    "score_dict_full={'random':[],'ddqn':[],'a2c':[],'sarsa':[]}\n",
    "border_med = 100\n",
    "EPISODES=140\n",
    "score_dict_med={'random':[],'ddqn':[],'a2c':[],'sarsa':[]}#c border_med по... такты. Надо, чтобы проверить быстроту обучения\n",
    "\n",
    "agent_list=[random_agent.randomAgent,a2c.A2CAgent,ddqn.DoubleDQNAgent,sarsa.SarsaAgent]\n",
    "for ag_num in range(4):\n",
    "    if ag_num==0:\n",
    "        name='random'\n",
    "    if ag_num==1:\n",
    "        name='a2c'\n",
    "    if ag_num==2:\n",
    "        name='ddqn' \n",
    "    if ag_num==3:\n",
    "        name='sarsa'\n",
    "    \n",
    "    for estimation in range(5):\n",
    "        print('_____',name,estimation)\n",
    "        #здесь весь код от инициализации модели до выдачи scores. Но без рендера.\n",
    "        # In case of CartPole-v1, maximum length of episode is 500\n",
    "        \n",
    "        env = aa_gun.AA_gun_simple0_env(fast_planes=True,long_projectiles=True,random_speed=True)\n",
    "        #env = gym.make('MountainCar-v0')\n",
    "        #env=CartPoleEnv9()\n",
    "        # get size of state and action from environment\n",
    "        state_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        #agent = DoubleDQNAgent(state_size, action_size)\n",
    "        agent = agent_list[ag_num](state_size, action_size)\n",
    "        agent.train_start=100\n",
    "        agent.render=True\n",
    "\n",
    "        scores, episodes = [], []\n",
    "        reward_lst = []\n",
    "        s_list=[]\n",
    "        a_list=[]\n",
    "\n",
    "        for e in range(EPISODES):\n",
    "            done = False\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "\n",
    "            while not done:\n",
    "                #if (e in range(2,7)) or (e in range(20,25)) or (e in range(100,103)) or (e in range(200,202)) or (e in range(300,306)) or (e in range(400,406)) or (e in range(500,506)) or (e in range(600,604)):\n",
    "                #    if agent.render:\n",
    "                #        env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                # if an action make the episode end, then gives penalty of -100\n",
    "\n",
    "                \n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                #if next_state[0,11]!=reward:\n",
    "                #    print('state[13]!=reward',state[0,11],reward)\n",
    "                #\n",
    "                s_list.append(state)\n",
    "                a_list.append(action)\n",
    "                reward_lst.append(reward)\n",
    "                #\n",
    "\n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    # every episode update the target model to be same with model\n",
    "                    agent.update_target_model()\n",
    "\n",
    "                    # every episode, plot the play time\n",
    "                    scores.append(score)\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, scores, 'b')\n",
    "                    #pylab.savefig(\"./save_graph/aa_gun_dqn.png\")\n",
    "                    print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                          len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "            # save the model\n",
    "            #if e % 50 == 0:\n",
    "            #    agent.model.save_weights(\"./save_model/aa_gun_dqn.h5\")\n",
    "\n",
    "\n",
    "\n",
    "        #и первые 3000 тактов - это рандом\n",
    "        #Ходов так 50\n",
    "        score_dict_full[name].append(np.mean(scores))\n",
    "        score_dict_med[name].append(np.mean(scores[border_med:]))\n",
    "        import pickle\n",
    "        f=open('score_dict.pkl','wb')\n",
    "        pickle.dump([score_dict_full,score_dict_med],f)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
