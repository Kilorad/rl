{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './env')\n",
    "sys.path.insert(1, './agents')\n",
    "\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import aa_gun\n",
    "import strategy_imitation, sarsa, model_based,clusterized_ql\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EPISODES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 0.5   memory length: 250   epsilon: 0.22212431388282694\n",
      "episode: 1   score: 0.75   memory length: 500   epsilon: 0.04933921081791662\n",
      "hit\n",
      "episode: 2   score: 1.8125   memory length: 750   epsilon: 0.010959438350449888\n",
      "hit\n",
      "episode: 3   score: 2.1875   memory length: 1000   epsilon: 0.009953376870880467\n",
      "episode: 4   score: 0.875   memory length: 1250   epsilon: 0.009953376870880467\n",
      "hit\n",
      "episode: 5   score: 1.5625   memory length: 1500   epsilon: 0.009953376870880467\n",
      "episode: 6   score: 0.875   memory length: 1750   epsilon: 0.009953376870880467\n",
      "episode: 7   score: 0.625   memory length: 2000   epsilon: 0.009953376870880467\n",
      "episode: 8   score: 0.5   memory length: 2250   epsilon: 0.009953376870880467\n",
      "episode: 9   score: 0.125   memory length: 2500   epsilon: 0.009953376870880467\n",
      "episode: 10   score: 0.8125   memory length: 2750   epsilon: 0.009953376870880467\n",
      "hit\n",
      "quality (0.2635750292347368, 0.34566679559962005)\n",
      "episode: 11   score: 2.5   memory length: 3000   epsilon: 0.009953376870880467\n",
      "quality (0.25189962052021303, 0.32225583145867814)\n",
      "episode: 12   score: 0   memory length: 3250   epsilon: 0.009953376870880467\n",
      "quality (0.2664680897981137, 0.2938041755866998)\n",
      "episode: 13   score: 0.0625   memory length: 3500   epsilon: 0.009953376870880467\n",
      "quality (0.27798747190974044, 0.41682081334443405)\n",
      "episode: 14   score: 0   memory length: 3750   epsilon: 0.009953376870880467\n",
      "quality (0.27490209513000374, 0.43557929104283166)\n",
      "episode: 15   score: 0.0625   memory length: 4000   epsilon: 0.009953376870880467\n",
      "quality (0.2852563624170915, 0.432628579931938)\n",
      "episode: 16   score: 0   memory length: 4250   epsilon: 0.009953376870880467\n",
      "quality (0.3049998036549018, 0.4392230731879054)\n",
      "episode: 17   score: 0   memory length: 4500   epsilon: 0.009953376870880467\n",
      "quality (0.30101908487997625, 0.4255620526867882)\n",
      "episode: 18   score: 0   memory length: 4750   epsilon: 0.009953376870880467\n",
      "quality (0.3138414730227082, 0.4285397929010955)\n",
      "episode: 19   score: 0   memory length: 5000   epsilon: 0.009953376870880467\n",
      "quality (0.3104480956783088, 0.4130800513181871)\n",
      "episode: 20   score: 0   memory length: 5250   epsilon: 0.009953376870880467\n",
      "quality (0.34889646805733926, 0.43083142012897113)\n",
      "episode: 21   score: 0   memory length: 5500   epsilon: 0.009953376870880467\n",
      "hit\n",
      "quality (0.347757397050882, 0.3937347761692439)\n",
      "episode: 22   score: 1.625   memory length: 5750   epsilon: 0.009953376870880467\n",
      "quality (0.4182544794358579, 0.4417033856062776)\n",
      "episode: 23   score: 0.0625   memory length: 6000   epsilon: 0.009953376870880467\n",
      "quality (0.48075180877543044, 0.4357866699889736)\n",
      "episode: 24   score: 0   memory length: 6250   epsilon: 0.009953376870880467\n",
      "quality (0.4512242641948806, 0.4304694093561589)\n",
      "episode: 25   score: 0   memory length: 6500   epsilon: 0.009953376870880467\n",
      "quality (0.5478185127260831, 0.4756431291823014)\n",
      "episode: 26   score: 0   memory length: 6750   epsilon: 0.009953376870880467\n",
      "quality (0.5507167703520286, 0.4619580297714959)\n",
      "episode: 27   score: 0   memory length: 7000   epsilon: 0.009953376870880467\n",
      "quality (0.5442139480812997, 0.467280736440021)\n",
      "episode: 28   score: 0   memory length: 7250   epsilon: 0.009953376870880467\n",
      "quality (0.6563247694012178, 0.4496812685762272)\n",
      "episode: 29   score: 0   memory length: 7500   epsilon: 0.009953376870880467\n",
      "quality (0.6512868481363892, 0.46802062075620243)\n",
      "episode: 30   score: 0   memory length: 7750   epsilon: 0.009953376870880467\n",
      "quality (0.736368352693805, 0.4745515015436713)\n",
      "episode: 31   score: 0   memory length: 8000   epsilon: 0.009953376870880467\n",
      "quality (0.8396051055576921, 0.4443566901911007)\n",
      "episode: 32   score: 0   memory length: 8250   epsilon: 0.009953376870880467\n",
      "quality (0.9882776644905431, 0.438614540709889)\n",
      "episode: 33   score: 0   memory length: 8500   epsilon: 0.009953376870880467\n",
      "quality (0.9922499196091014, 0.4405358094689321)\n",
      "episode: 34   score: 0   memory length: 8750   epsilon: 0.009953376870880467\n",
      "quality (0.9948036491312151, 0.4413992585279378)\n",
      "episode: 35   score: 0   memory length: 9000   epsilon: 0.009953376870880467\n",
      "quality (1.0259259651590174, 0.4370781789156556)\n",
      "episode: 36   score: 0   memory length: 9250   epsilon: 0.009953376870880467\n",
      "quality (1.0153346786487984, 0.45745284565915456)\n",
      "episode: 37   score: 0   memory length: 9500   epsilon: 0.009953376870880467\n",
      "quality (1.0330150237009674, 0.43455090097220905)\n",
      "episode: 38   score: 0   memory length: 9750   epsilon: 0.009953376870880467\n",
      "quality (1.0339648639526966, 0.44428172348519474)\n",
      "episode: 39   score: 0   memory length: 10000   epsilon: 0.009953376870880467\n",
      "quality (1.0363514131004723, 0.4398722159802083)\n",
      "episode: 40   score: 0   memory length: 10250   epsilon: 0.009953376870880467\n",
      "quality (1.0350392916331332, 0.44363861384402464)\n",
      "episode: 41   score: 0   memory length: 10500   epsilon: 0.009953376870880467\n",
      "quality (1.0404139990069303, 0.43831646339384556)\n",
      "episode: 42   score: 0   memory length: 10750   epsilon: 0.009953376870880467\n",
      "quality (1.0410889664004654, 0.444101088678992)\n",
      "episode: 43   score: 0.5   memory length: 11000   epsilon: 0.009953376870880467\n",
      "quality (1.0440807593219306, 0.4196091828921662)\n",
      "episode: 44   score: 0   memory length: 11250   epsilon: 0.009953376870880467\n",
      "quality (1.0429001502391881, 0.43209905626875733)\n",
      "episode: 45   score: 0   memory length: 11500   epsilon: 0.009953376870880467\n",
      "quality (1.0464356551724767, 0.4098257196966129)\n",
      "episode: 46   score: 0   memory length: 11750   epsilon: 0.009953376870880467\n",
      "quality (1.0467956656998774, 0.39987880445308976)\n",
      "episode: 47   score: 0   memory length: 12000   epsilon: 0.009953376870880467\n",
      "quality (1.046679317373295, 0.3917583773500321)\n",
      "episode: 48   score: 0   memory length: 12250   epsilon: 0.009953376870880467\n",
      "quality (1.0485843064402416, 0.39831870115945484)\n",
      "episode: 49   score: 0   memory length: 12500   epsilon: 0.009953376870880467\n",
      "quality (1.0481819509865735, 0.40803358941486173)\n",
      "episode: 50   score: 0   memory length: 12750   epsilon: 0.009953376870880467\n",
      "quality (1.0497189622468646, 0.3941814970071552)\n",
      "episode: 51   score: 0   memory length: 13000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "quality (1.02254420069586, 0.42040031021814384)\n",
      "episode: 52   score: 1.25   memory length: 13250   epsilon: 0.009953376870880467\n",
      "quality (1.0243382632005071, 0.44362418300352563)\n",
      "episode: 53   score: 0   memory length: 13500   epsilon: 0.009953376870880467\n",
      "quality (1.0242965256924277, 0.4439822079395507)\n",
      "episode: 54   score: 0   memory length: 13750   epsilon: 0.009953376870880467\n",
      "quality (1.0247291171082904, 0.44188053271034755)\n",
      "episode: 55   score: 0   memory length: 14000   epsilon: 0.009953376870880467\n",
      "quality (1.0240596908940822, 0.43881181052844465)\n",
      "episode: 56   score: 0   memory length: 14250   epsilon: 0.009953376870880467\n",
      "quality (1.0227432610745368, 0.4379722859088897)\n",
      "episode: 57   score: 0   memory length: 14500   epsilon: 0.009953376870880467\n",
      "quality (1.0230223963744942, 0.42405999190305027)\n",
      "episode: 58   score: 0   memory length: 14750   epsilon: 0.009953376870880467\n",
      "quality (1.0228721484821894, 0.41858444134691253)\n",
      "episode: 59   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.023968957810187, 0.4327591341034992)\n",
      "episode: 60   score: 0.125   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0151231873764204, 0.39953961899618484)\n",
      "episode: 61   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (0.9984019425865968, 0.4331216683235611)\n",
      "episode: 62   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0101760399051454, 0.46671645411082296)\n",
      "episode: 63   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0100205356979186, 0.4490835078168489)\n",
      "episode: 64   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0065727171740853, 0.42771932945557545)\n",
      "episode: 65   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (0.9943262447658618, 0.4613726012455112)\n",
      "episode: 66   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.000116658362052, 0.39913613015356597)\n",
      "episode: 67   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (0.9927450946530383, 0.4647451473924587)\n",
      "episode: 68   score: 1.5   memory length: 15000   epsilon: 0.009953376870880467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality (1.00092712269018, 0.4558238953898541)\n",
      "episode: 69   score: 1.75   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "quality (0.9948751927815479, 0.4652224683011588)\n",
      "episode: 70   score: 3.125   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (0.9896580168573755, 0.40630737568734715)\n",
      "episode: 71   score: 1.8125   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "hit\n",
      "quality (0.9849865487209275, 0.4181470627723526)\n",
      "episode: 72   score: 3.0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "quality (0.9876301392612097, 0.450299720748507)\n",
      "episode: 73   score: 3.25   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "hit\n",
      "quality (0.9900258001151809, 0.4405377379454488)\n",
      "episode: 74   score: 3.3125   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "quality (0.9984553848260823, 0.42963940762800046)\n",
      "episode: 75   score: 2.375   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "hit\n",
      "quality (1.0024594223012486, 0.3814384610263444)\n",
      "episode: 76   score: 2.75   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (0.9972859237001069, 0.36482419231701785)\n",
      "episode: 77   score: 1.75   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "quality (0.979278088368402, 0.4217932256767539)\n",
      "episode: 78   score: 1.9375   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (0.9962334881028538, 0.3774670340255852)\n",
      "episode: 79   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "hit\n",
      "quality (1.04478399430996, 0.37794224998017195)\n",
      "episode: 80   score: 3.625   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.042323404931913, 0.3967846147360571)\n",
      "episode: 81   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0425511944991763, 0.37384592205185296)\n",
      "episode: 82   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (0.9936890168669411, 0.3753093782743487)\n",
      "episode: 83   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "hit\n",
      "quality (1.0180934899707639, 0.3703948591080045)\n",
      "episode: 84   score: 3.3125   memory length: 15000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "hit\n",
      "quality (1.0580731479883694, 0.41026506366931104)\n",
      "episode: 85   score: 3.25   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.006337925712527, 0.3742522384559802)\n",
      "episode: 86   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0604367177335596, 0.3938898926547976)\n",
      "episode: 87   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0647375217141988, 0.3876512104871871)\n",
      "episode: 88   score: 0.0625   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0669170856703554, 0.3967180109120956)\n",
      "episode: 89   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0637684714178763, 0.4074613278456737)\n",
      "episode: 90   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0672735066208183, 0.4149057000260498)\n",
      "episode: 91   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n",
      "quality (1.0658496039865968, 0.4130928808205129)\n",
      "episode: 92   score: 0   memory length: 15000   epsilon: 0.009953376870880467\n"
     ]
    }
   ],
   "source": [
    "# In case of CartPole-v1, maximum length of episode is 500\n",
    "env = aa_gun.AA_gun_simple0_env()\n",
    "#env = gym.make('AirRaid-ram-v0')\n",
    "#env = gym.make('Robotank-ramNoFrameskip-v0')\n",
    "#env = gym.make('Seaquest-ramNoFrameskip-v0')\n",
    "#env=CartPoleEnv9()\n",
    "# get size of state and action from environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "#agent = DoubleDQNAgent(state_size, action_size)\n",
    "#agent = model_based.ModelBasedAgent(state_size, action_size)\n",
    "agent = clusterized_ql.ClusterQLAgent(state_size, action_size)\n",
    "agent.train_start=3000\n",
    "#agent = strategy_imitation.ImitAgent(state_size, action_size)\n",
    "agent.render=True\n",
    "\n",
    "scores, episodes = [], []\n",
    "reward_lst = []\n",
    "s_list=[]\n",
    "a_list=[]\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "        if (e in range(100,300)) or (e in range(300,306)) or (e in range(400,406)) or (e in range(500,506)) or (e in range(600,604)):\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "        # get action for the current state and go one step in environment\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "        \n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        reward_curr=reward\n",
    "        #if done:\n",
    "        #    reward_curr -= 100\n",
    "        agent.append_sample(state, action, reward_curr, next_state, done)\n",
    "        #if next_state[0,11]!=reward:\n",
    "        #    print('state[13]!=reward',state[0,11],reward)\n",
    "        #\n",
    "        s_list.append(state)\n",
    "        a_list.append(action)\n",
    "        reward_lst.append(reward)\n",
    "        #\n",
    "        \n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, scores, 'b')\n",
    "            #pylab.savefig(\"./save_graph/aa_gun_dqn.png\")\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.s), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 490\n",
    "            # stop training\n",
    "\n",
    "    # save the model\n",
    "    #if e % 50 == 0:\n",
    "    #    agent.model.save_weights(\"./save_model/aa_gun_dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.sans_mn[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.saq_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.nsr_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sar_table(s,a,r):\n",
    "    #print(np.array(a,ndmin=2).T.shape)\n",
    "    #print(np.array(r,ndmin=2).T.shape)\n",
    "    #print(np.array(s,ndmin=2)[:,0,:].shape)\n",
    "    return np.hstack( (np.array(s,ndmin=2)[:,0,:],np.array(a,ndmin=2).T,np.array(r,ndmin=2).T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(np.mean(reward_lst))\n",
    "plt.plot(reward_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.r_disco)\n",
    "#plt.plot(agent.d*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(reward_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Чтобы анализировать разрешимость задачи\n",
    "def replicate_reward(sar,border=0,wanted_part=0.5):\n",
    "    part = np.mean(sar[:,-1:]>border)\n",
    "    if part==0:\n",
    "        print('ERROR')\n",
    "        return(sar)\n",
    "    else:\n",
    "        while part<wanted_part:\n",
    "            sar=np.vstack((sar,sar[np.where(sar[:,-1:]>border)[0],:]))\n",
    "            part = np.mean(sar[:,-1:]>border)\n",
    "        print(part)\n",
    "        return(sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar=make_sar_table(s_list,a_list,reward_lst)\n",
    "sar=replicate_reward(sar)\n",
    "X=sar[1:,:]\n",
    "Y=sar[:-1,:]\n",
    "Y=sar[:-1,-1:]\n",
    "Y=sar[1:,-1:]\n",
    "sar_width=X.shape[1]\n",
    "nn = Sequential()\n",
    "nn.add(Dense(200, input_dim=sar_width, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(200, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dense(200, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(BatchNormalization())\n",
    "#nn.add(Dense(sar_width, activation='linear',\n",
    "#                kernel_initializer='he_uniform'))\n",
    "nn.add(Dense(1, activation='linear',\n",
    "                kernel_initializer='he_uniform'))\n",
    "\n",
    "nn.summary()\n",
    "nn.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "l=X.shape[0]\n",
    "X_train=X[:int(l/2),:]\n",
    "Y_train=Y[:int(l/2),:]\n",
    "X_test=X[int(l/2):,:]\n",
    "Y_test=Y[int(l/2):,:]\n",
    "nn.fit(X_train, Y_train, batch_size=1200,epochs=30000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=np.array(nn.predict(X_test),ndmin=2)\n",
    "mse = np.mean((Y_pred-Y_test)**2)\n",
    "print(mse)\n",
    "rmae=np.mean(np.abs(Y_pred-Y_test))/np.mean(np.abs(Y_test))\n",
    "print(rmae)\n",
    "rmae_diversed=np.mean(np.abs(Y_pred-Y_test),axis=0)/np.mean(np.abs(Y_test),axis=0)\n",
    "print(rmae_diversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnumn_num=-1\n",
    "plt.plot(Y_pred[:3000,colnumn_num])\n",
    "plt.plot(Y_test[:3000,colnumn_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train (переобучение?)\n",
    "Y_pred=np.array(nn.predict(X_train),ndmin=2)\n",
    "mse = np.mean((Y_pred-Y_train)**2)\n",
    "print(mse)\n",
    "rmae=np.mean(np.abs(Y_pred-Y_train))/np.mean(np.abs(Y_train))\n",
    "print(rmae)\n",
    "rmae_diversed=np.mean(np.abs(Y_pred-Y_train),axis=0)/np.mean(np.abs(Y_train),axis=0)\n",
    "print(rmae_diversed)\n",
    "\n",
    "colnumn_num=-1\n",
    "plt.plot(Y_pred[:1300,colnumn_num])\n",
    "plt.plot(Y_train[:1300,colnumn_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgbparams = {\n",
    "    'booster':'gbtree',\n",
    "    'metric':'mse',\n",
    "    'objective':'reg:squarederror',\n",
    "    'verbosity':0,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 90,\n",
    "    'eta': 0.3,\n",
    "    'nthreads': 2,\n",
    "    'seed':0\n",
    "}\n",
    "nn=xgb.XGBRegressor(**xgbparams)\n",
    "nn.fit(X_train[:int(l/4),:], Y_train[:int(l/4),:],\n",
    "           eval_set=[(X_train[int(l/4):,:], Y_train[int(l/4):,:])],\n",
    "           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(sar)\n",
    "df[df[13]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
