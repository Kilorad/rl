{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './env')\n",
    "sys.path.insert(1, './agents')\n",
    "\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import aa_gun\n",
    "import strategy_imitation, sarsa, model_based,clusterized_ql\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EPISODES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 0.5625   memory length: 250   epsilon: 0.22212431388282694\n",
      "episode: 1   score: 0.25   memory length: 500   epsilon: 0.04933921081791662\n",
      "episode: 2   score: 0   memory length: 750   epsilon: 0.010959438350449888\n",
      "episode: 3   score: 0.5625   memory length: 1000   epsilon: 0.009953376870880467\n",
      "episode: 4   score: 0.1875   memory length: 1250   epsilon: 0.009953376870880467\n",
      "hit\n",
      "episode: 5   score: 2.125   memory length: 1500   epsilon: 0.009953376870880467\n",
      "hit\n",
      "episode: 6   score: 1.75   memory length: 1750   epsilon: 0.009953376870880467\n",
      "hit\n",
      "episode: 7   score: 1.625   memory length: 2000   epsilon: 0.009953376870880467\n",
      "hit\n",
      "episode: 8   score: 1.625   memory length: 2250   epsilon: 0.009953376870880467\n",
      "hit\n",
      "episode: 9   score: 1.8125   memory length: 2500   epsilon: 0.009953376870880467\n",
      "hit\n",
      "episode: 10   score: 2.25   memory length: 2750   epsilon: 0.009953376870880467\n",
      "quality (0.5455417031514578, 1.0000000000000002)\n",
      "episode: 11   score: 0.3125   memory length: 3000   epsilon: 0.009953376870880467\n",
      "quality (0.594069637281746, 0.9999999999999998)\n",
      "episode: 12   score: 0   memory length: 3250   epsilon: 0.009953376870880467\n",
      "quality (0.6559781497743935, 1.0)\n",
      "episode: 13   score: 0   memory length: 3500   epsilon: 0.009953376870880467\n",
      "quality (0.6564224375501477, 0.9999999999999999)\n",
      "episode: 14   score: 0   memory length: 3750   epsilon: 0.009953376870880467\n",
      "quality (0.6438982358948351, 1.0)\n",
      "episode: 15   score: 0.0625   memory length: 4000   epsilon: 0.009953376870880467\n",
      "quality (0.6428315012905012, 1.0)\n",
      "episode: 16   score: 0   memory length: 4250   epsilon: 0.009953376870880467\n",
      "quality (0.6416427437137449, 0.9999999999999998)\n",
      "episode: 17   score: 0   memory length: 4500   epsilon: 0.009953376870880467\n",
      "quality (0.6430173275280108, 1.0)\n",
      "episode: 18   score: 0   memory length: 4750   epsilon: 0.009953376870880467\n",
      "quality (0.7164406373347997, 1.0)\n",
      "episode: 19   score: 0   memory length: 5000   epsilon: 0.009953376870880467\n",
      "quality (0.7182078269499744, 1.0000000000000002)\n",
      "episode: 20   score: 0   memory length: 5250   epsilon: 0.009953376870880467\n",
      "quality (0.8130905498741869, 1.0)\n",
      "episode: 21   score: 0   memory length: 5500   epsilon: 0.009953376870880467\n",
      "quality (0.8235893499651896, 0.9999999999999999)\n",
      "episode: 22   score: 0   memory length: 5750   epsilon: 0.009953376870880467\n",
      "quality (0.8214779970498048, 1.0000000000000002)\n",
      "episode: 23   score: 0   memory length: 6000   epsilon: 0.009953376870880467\n",
      "quality (0.829592843205555, 1.0)\n",
      "episode: 24   score: 0   memory length: 6250   epsilon: 0.009953376870880467\n",
      "quality (0.8227894196051421, 1.0000000000000002)\n",
      "episode: 25   score: 0   memory length: 6500   epsilon: 0.009953376870880467\n",
      "quality (0.8212963564987578, 0.9999999999999999)\n",
      "episode: 26   score: 0.25   memory length: 6750   epsilon: 0.009953376870880467\n",
      "quality (0.8199003398872644, 1.0000000000000002)\n",
      "episode: 27   score: 0   memory length: 7000   epsilon: 0.009953376870880467\n",
      "quality (0.8243209420727149, 0.9999999999999999)\n",
      "episode: 28   score: 0.25   memory length: 7000   epsilon: 0.009953376870880467\n",
      "quality (0.8329451283932277, 1.0000000000000002)\n",
      "episode: 29   score: 0   memory length: 7000   epsilon: 0.009953376870880467\n",
      "quality (0.8192288453656017, 1.0000000000000002)\n",
      "episode: 30   score: 0.125   memory length: 7000   epsilon: 0.009953376870880467\n"
     ]
    }
   ],
   "source": [
    "# In case of CartPole-v1, maximum length of episode is 500\n",
    "env = aa_gun.AA_gun_simple0_env()\n",
    "#env = gym.make('AirRaid-ram-v0')\n",
    "#env = gym.make('Robotank-ramNoFrameskip-v0')\n",
    "#env = gym.make('Seaquest-ramNoFrameskip-v0')\n",
    "#env=CartPoleEnv9()\n",
    "# get size of state and action from environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "#agent = DoubleDQNAgent(state_size, action_size)\n",
    "#agent = model_based.ModelBasedAgent(state_size, action_size)\n",
    "agent = clusterized_ql.ClusterQLAgent(state_size, action_size)\n",
    "agent.train_start=3000\n",
    "#agent = strategy_imitation.ImitAgent(state_size, action_size)\n",
    "agent.render=True\n",
    "\n",
    "scores, episodes = [], []\n",
    "reward_lst = []\n",
    "s_list=[]\n",
    "a_list=[]\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "        if (e in range(100,300)) or (e in range(300,306)) or (e in range(400,406)) or (e in range(500,506)) or (e in range(600,604)):\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "        # get action for the current state and go one step in environment\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "        \n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        reward_curr=reward\n",
    "        #if done:\n",
    "        #    reward_curr -= 100\n",
    "        agent.append_sample(state, action, reward_curr, next_state, done)\n",
    "        #if next_state[0,11]!=reward:\n",
    "        #    print('state[13]!=reward',state[0,11],reward)\n",
    "        #\n",
    "        s_list.append(state)\n",
    "        a_list.append(action)\n",
    "        reward_lst.append(reward)\n",
    "        #\n",
    "        \n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, scores, 'b')\n",
    "            #pylab.savefig(\"./save_graph/aa_gun_dqn.png\")\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.s), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 490\n",
    "            # stop training\n",
    "\n",
    "    # save the model\n",
    "    #if e % 50 == 0:\n",
    "    #    agent.model.save_weights(\"./save_model/aa_gun_dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sar_table(s,a,r):\n",
    "    #print(np.array(a,ndmin=2).T.shape)\n",
    "    #print(np.array(r,ndmin=2).T.shape)\n",
    "    #print(np.array(s,ndmin=2)[:,0,:].shape)\n",
    "    return np.hstack( (np.array(s,ndmin=2)[:,0,:],np.array(a,ndmin=2).T,np.array(r,ndmin=2).T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(np.mean(reward_lst))\n",
    "plt.plot(reward_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.r_disco)\n",
    "#plt.plot(agent.d*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(reward_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Чтобы анализировать разрешимость задачи\n",
    "def replicate_reward(sar,border=0,wanted_part=0.5):\n",
    "    part = np.mean(sar[:,-1:]>border)\n",
    "    if part==0:\n",
    "        print('ERROR')\n",
    "        return(sar)\n",
    "    else:\n",
    "        while part<wanted_part:\n",
    "            sar=np.vstack((sar,sar[np.where(sar[:,-1:]>border)[0],:]))\n",
    "            part = np.mean(sar[:,-1:]>border)\n",
    "        print(part)\n",
    "        return(sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar=make_sar_table(s_list,a_list,reward_lst)\n",
    "sar=replicate_reward(sar)\n",
    "X=sar[1:,:]\n",
    "Y=sar[:-1,:]\n",
    "Y=sar[:-1,-1:]\n",
    "Y=sar[1:,-1:]\n",
    "sar_width=X.shape[1]\n",
    "nn = Sequential()\n",
    "nn.add(Dense(200, input_dim=sar_width, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(200, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dense(200, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(BatchNormalization())\n",
    "#nn.add(Dense(sar_width, activation='linear',\n",
    "#                kernel_initializer='he_uniform'))\n",
    "nn.add(Dense(1, activation='linear',\n",
    "                kernel_initializer='he_uniform'))\n",
    "\n",
    "nn.summary()\n",
    "nn.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "l=X.shape[0]\n",
    "X_train=X[:int(l/2),:]\n",
    "Y_train=Y[:int(l/2),:]\n",
    "X_test=X[int(l/2):,:]\n",
    "Y_test=Y[int(l/2):,:]\n",
    "nn.fit(X_train, Y_train, batch_size=1200,epochs=30000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=np.array(nn.predict(X_test),ndmin=2)\n",
    "mse = np.mean((Y_pred-Y_test)**2)\n",
    "print(mse)\n",
    "rmae=np.mean(np.abs(Y_pred-Y_test))/np.mean(np.abs(Y_test))\n",
    "print(rmae)\n",
    "rmae_diversed=np.mean(np.abs(Y_pred-Y_test),axis=0)/np.mean(np.abs(Y_test),axis=0)\n",
    "print(rmae_diversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnumn_num=-1\n",
    "plt.plot(Y_pred[:3000,colnumn_num])\n",
    "plt.plot(Y_test[:3000,colnumn_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train (переобучение?)\n",
    "Y_pred=np.array(nn.predict(X_train),ndmin=2)\n",
    "mse = np.mean((Y_pred-Y_train)**2)\n",
    "print(mse)\n",
    "rmae=np.mean(np.abs(Y_pred-Y_train))/np.mean(np.abs(Y_train))\n",
    "print(rmae)\n",
    "rmae_diversed=np.mean(np.abs(Y_pred-Y_train),axis=0)/np.mean(np.abs(Y_train),axis=0)\n",
    "print(rmae_diversed)\n",
    "\n",
    "colnumn_num=-1\n",
    "plt.plot(Y_pred[:1300,colnumn_num])\n",
    "plt.plot(Y_train[:1300,colnumn_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgbparams = {\n",
    "    'booster':'gbtree',\n",
    "    'metric':'mse',\n",
    "    'objective':'reg:squarederror',\n",
    "    'verbosity':0,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 90,\n",
    "    'eta': 0.3,\n",
    "    'nthreads': 2,\n",
    "    'seed':0\n",
    "}\n",
    "nn=xgb.XGBRegressor(**xgbparams)\n",
    "nn.fit(X_train[:int(l/4),:], Y_train[:int(l/4),:],\n",
    "           eval_set=[(X_train[int(l/4):,:], Y_train[int(l/4):,:])],\n",
    "           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(sar)\n",
    "df[df[13]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
