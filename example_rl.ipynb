{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './env')\n",
    "sys.path.insert(1, './agents')\n",
    "\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import aa_gun\n",
    "import strategy_imitation, sarsa\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPISODES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/s.dovgan/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:72: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s.dovgan/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:515: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s.dovgan/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4048: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s.dovgan/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s.dovgan/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3368: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 160)               2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 29,281\n",
      "Trainable params: 28,641\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/s.dovgan/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:782: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 160)               3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 160)               25760     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 160)               640       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 30,401\n",
      "Trainable params: 29,761\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "hit\n",
      "episode: 0   score: 1.875   memory length: 250   epsilon: 0.7787033741169904\n",
      "hit\n",
      "episode: 1   score: 1.5625   memory length: 500   epsilon: 0.6063789448611848\n",
      "episode: 2   score: 0.4375   memory length: 750   epsilon: 0.47218933035690447\n",
      "hit\n",
      "hit\n",
      "episode: 3   score: 2.625   memory length: 1000   epsilon: 0.3676954247709635\n",
      "episode: 4   score: 0.0625   memory length: 1250   epsilon: 0.28632566791652947\n",
      "episode: 5   score: 0.75   memory length: 1500   epsilon: 0.22296276370290227\n",
      "episode: 6   score: 0.4375   memory length: 1750   epsilon: 0.17362185639789907\n",
      "episode: 7   score: 0.8125   memory length: 2000   epsilon: 0.1351999253974994\n",
      "hit\n",
      "episode: 8   score: 1.375   memory length: 2250   epsilon: 0.10528063808739813\n",
      "episode: 9   score: 0   memory length: 2500   epsilon: 0.08198238810784661\n",
      "episode: 10   score: 0.6875   memory length: 2750   epsilon: 0.06383996223774882\n",
      "WARNING:tensorflow:From /home/s.dovgan/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:984: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.6073\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.5851\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.5777\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.5380\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 5us/step - loss: 3.5315\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.5202\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.5007\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.4835\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.4679\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.4639\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 5us/step - loss: 3.2068\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.1850\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.1547\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.1395\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 7us/step - loss: 3.1237\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.1074\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 7us/step - loss: 3.0950\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 7us/step - loss: 3.0867\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 6us/step - loss: 3.0559\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 7us/step - loss: 3.0351\n",
      "episode: 11   score: 1.0625   memory length: 3000   epsilon: 0.04971239399803625\n",
      "Epoch 1/1\n",
      "3250/3250 [==============================] - 0s 6us/step - loss: 0.0630\n",
      "Epoch 1/1\n",
      "3250/3250 [==============================] - 0s 7us/step - loss: 0.0395\n",
      "episode: 12   score: 0   memory length: 3250   epsilon: 0.03871120894170404\n",
      "Epoch 1/1\n",
      "3500/3500 [==============================] - 0s 6us/step - loss: 0.0512\n",
      "Epoch 1/1\n",
      "3500/3500 [==============================] - 0s 6us/step - loss: 0.0272\n",
      "episode: 13   score: 0.25   memory length: 3500   epsilon: 0.030144549019052724\n",
      "Epoch 1/1\n",
      "3750/3750 [==============================] - 0s 5us/step - loss: 0.0581\n",
      "Epoch 1/1\n",
      "3750/3750 [==============================] - 0s 6us/step - loss: 0.0329\n",
      "episode: 14   score: 0.625   memory length: 3750   epsilon: 0.023473662032371355\n",
      "hit\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 0s 5us/step - loss: 0.0540\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 0s 6us/step - loss: 0.0250\n",
      "episode: 15   score: 1   memory length: 4000   epsilon: 0.018279019827489446\n"
     ]
    }
   ],
   "source": [
    "# In case of CartPole-v1, maximum length of episode is 500\n",
    "env = aa_gun.AA_gun_simple0_env()\n",
    "#env=CartPoleEnv9()\n",
    "# get size of state and action from environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "#agent = DoubleDQNAgent(state_size, action_size)\n",
    "agent = sarsa.SarsaAgent(state_size, action_size)\n",
    "#agent = strategy_imitation.ImitAgent(state_size, action_size)\n",
    "agent.render=False\n",
    "\n",
    "scores, episodes = [], []\n",
    "reward_lst = []\n",
    "s_list=[]\n",
    "a_list=[]\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "        if (e in range(2,7)) or (e in range(20,25)) or (e in range(100,103)) or (e in range(200,202)) or (e in range(300,306)) or (e in range(400,406)) or (e in range(500,506)) or (e in range(600,604)):\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "        # get action for the current state and go one step in environment\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "        \n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        #if next_state[0,11]!=reward:\n",
    "        #    print('state[13]!=reward',state[0,11],reward)\n",
    "        #\n",
    "        s_list.append(state)\n",
    "        a_list.append(action)\n",
    "        reward_lst.append(reward)\n",
    "        #\n",
    "        \n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, scores, 'b')\n",
    "            #pylab.savefig(\"./save_graph/aa_gun_dqn.png\")\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 490\n",
    "            # stop training\n",
    "\n",
    "    # save the model\n",
    "    #if e % 50 == 0:\n",
    "    #    agent.model.save_weights(\"./save_model/aa_gun_dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sar_table(s,a,r):\n",
    "    #print(np.array(a,ndmin=2).T.shape)\n",
    "    #print(np.array(r,ndmin=2).T.shape)\n",
    "    #print(np.array(s,ndmin=2)[:,0,:].shape)\n",
    "    return np.hstack( (np.array(s,ndmin=2)[:,0,:],np.array(a,ndmin=2).T,np.array(r,ndmin=2).T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(np.mean(reward_lst))\n",
    "plt.plot(reward_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.r_disco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(reward_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Чтобы анализировать разрешимость задачи\n",
    "def replicate_reward(sar,border=0,wanted_part=0.5):\n",
    "    part = np.mean(sar[:,-1:]>border)\n",
    "    if part==0:\n",
    "        print('ERROR')\n",
    "        return(sar)\n",
    "    else:\n",
    "        while part<wanted_part:\n",
    "            sar=np.vstack((sar,sar[np.where(sar[:,-1:]>border)[0],:]))\n",
    "            part = np.mean(sar[:,-1:]>border)\n",
    "        print(part)\n",
    "        return(sar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar=make_sar_table(s_list,a_list,reward_lst)\n",
    "sar=replicate_reward(sar)\n",
    "X=sar[1:,:]\n",
    "Y=sar[:-1,:]\n",
    "Y=sar[:-1,-1:]\n",
    "Y=sar[1:,-1:]\n",
    "sar_width=X.shape[1]\n",
    "nn = Sequential()\n",
    "nn.add(Dense(200, input_dim=sar_width, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Dense(200, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dense(200, activation='relu',\n",
    "                kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(BatchNormalization())\n",
    "#nn.add(Dense(sar_width, activation='linear',\n",
    "#                kernel_initializer='he_uniform'))\n",
    "nn.add(Dense(1, activation='linear',\n",
    "                kernel_initializer='he_uniform'))\n",
    "\n",
    "nn.summary()\n",
    "nn.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "l=X.shape[0]\n",
    "X_train=X[:int(l/2),:]\n",
    "Y_train=Y[:int(l/2),:]\n",
    "X_test=X[int(l/2):,:]\n",
    "Y_test=Y[int(l/2):,:]\n",
    "nn.fit(X_train, Y_train, batch_size=1200,epochs=30000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=np.array(nn.predict(X_test),ndmin=2)\n",
    "mse = np.mean((Y_pred-Y_test)**2)\n",
    "print(mse)\n",
    "rmae=np.mean(np.abs(Y_pred-Y_test))/np.mean(np.abs(Y_test))\n",
    "print(rmae)\n",
    "rmae_diversed=np.mean(np.abs(Y_pred-Y_test),axis=0)/np.mean(np.abs(Y_test),axis=0)\n",
    "print(rmae_diversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnumn_num=-1\n",
    "plt.plot(Y_pred[:3000,colnumn_num])\n",
    "plt.plot(Y_test[:3000,colnumn_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train (переобучение?)\n",
    "Y_pred=np.array(nn.predict(X_train),ndmin=2)\n",
    "mse = np.mean((Y_pred-Y_train)**2)\n",
    "print(mse)\n",
    "rmae=np.mean(np.abs(Y_pred-Y_train))/np.mean(np.abs(Y_train))\n",
    "print(rmae)\n",
    "rmae_diversed=np.mean(np.abs(Y_pred-Y_train),axis=0)/np.mean(np.abs(Y_train),axis=0)\n",
    "print(rmae_diversed)\n",
    "\n",
    "colnumn_num=-1\n",
    "plt.plot(Y_pred[:1300,colnumn_num])\n",
    "plt.plot(Y_train[:1300,colnumn_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgbparams = {\n",
    "    'booster':'gbtree',\n",
    "    'metric':'mse',\n",
    "    'objective':'reg:squarederror',\n",
    "    'verbosity':0,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 90,\n",
    "    'eta': 0.3,\n",
    "    'nthreads': 2,\n",
    "    'seed':0\n",
    "}\n",
    "nn=xgb.XGBRegressor(**xgbparams)\n",
    "nn.fit(X_train[:int(l/4),:], Y_train[:int(l/4),:],\n",
    "           eval_set=[(X_train[int(l/4):,:], Y_train[int(l/4):,:])],\n",
    "           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(sar)\n",
    "df[df[13]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
