{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, './env')\n",
    "sys.path.insert(1, './agents')\n",
    "\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import strategy_imitation, sarsa, ddqn, random_agent, a2c, model_based,graph_ai\n",
    "import aa_gun,jet_table_simple\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#как сделать запись в файл? Перегони в картинку, её перегони в верный формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(224, 224, 3) - размеры resnet101, resnet 101v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_square = np.zeros((224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ 2020-08-07 21:09:24.781831\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 23,901\n",
      "Trainable params: 23,501\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               14700     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 25,701\n",
      "Trainable params: 25,301\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 15664.8809\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3193.2534\n",
      "delta_r 74.40221204874142 r 441.88547156209495\n",
      "episode: 0   score: 9000.0 9000.0   epsilon: 0.009998671593271896 t: 7146\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21077.5254\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4955.2476\n",
      "delta_r 92.33905358566695 r 441.52204981812275\n",
      "episode: 1   score: 9000.0 9000.0   epsilon: 0.009998671593271896 t: 14100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-29c550638719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# every time step do the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/rl/agents/sarsa.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, epochs, sub_batch_size, verbose)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msub_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0msub_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_discounted_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/rl/agents/sarsa.py\u001b[0m in \u001b[0;36mmake_discounted_rewards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_disco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_smooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplanning_horison\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# pick samples randomly from replay memory (with batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/rl/common/utils.py\u001b[0m in \u001b[0;36mexp_smooth\u001b[0;34m(data, alpha, steps, dones)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mroll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mroll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mdata_to_mod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_to_mod\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#попытка масштабирования: нормировка\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Проверь на зенитке, на cartpole и на mountain car\n",
    "EPISODES=1400\n",
    "\n",
    "print('_____',pd.Timestamp.now())\n",
    "#здесь весь код от инициализации модели до выдачи scores.\n",
    "# In case of CartPole-v1, maximum length of episode is 500\n",
    "\n",
    "#env = jet_table_simple.jet_table_simple_env()\n",
    "#env = gym.make('Seaquest-ramNoFrameskip-v0')\n",
    "env = gym.make('BattleZone-ram-v0')\n",
    "#env=CartPoleEnv9()\n",
    "#env = aa_gun.AA_gun_simple0_env()\n",
    "# get size of state and action from environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = sarsa.SarsaAgent(state_size, action_size)\n",
    "#goal_state_list = [[100, 0, 0],[0, 0.1, -0.1],[2000, 0, 1]]\n",
    "#agent = graph_ai.GraphAI(state_size, action_size, goal_state_list=goal_state_list)\n",
    "agent.train_start=700\n",
    "#agent.epsilon_decay=0.9999\n",
    "agent.render=True\n",
    "\n",
    "scores, episodes = [], []\n",
    "reward_lst = []\n",
    "s_list=[]\n",
    "a_list=[]\n",
    "movie = []\n",
    "agent.epsilon = 1\n",
    "\n",
    "t = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        t += 1\n",
    "        if 1:\n",
    "            #, screen_width=224, screen_height=224\n",
    "            env.render(mode='human')\n",
    "            #rgb = env.render(mode='rgb_array', screen_width=224, screen_height=224)\n",
    "            rgb = env.render(mode='rgb_array')\n",
    "            shp = np.shape(rgb)\n",
    "            delta_x = int((224- shp[0])/2)\n",
    "            delta_y = int((224- shp[1])/2)\n",
    "            img = copy.copy(black_square)\n",
    "            img[delta_x:delta_x+shp[0],delta_y:delta_y+shp[1],:] = rgb\n",
    "            img = rgb\n",
    "\n",
    "            movie.append(img)\n",
    "            if len(movie)>1000:\n",
    "                hsh = str(int(np.random.rand()*1e9))\n",
    "                nam = f'battlezone_sarsa_{hsh}.pkl'\n",
    "                with open('./movies_save_raw/'+nam, 'wb') as f:\n",
    "                    pickle.dump(movie, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                movie = []\n",
    "                    \n",
    "\n",
    "        # get action for the current state and go one step in environment)\n",
    "        action = agent.get_action(state)\n",
    "         \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "            \n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        #if next_state[0,11]!=reward:\n",
    "        #    print('state[13]!=reward',state[0,11],reward)\n",
    "        #\n",
    "        s_list.append(state)\n",
    "        a_list.append(action)\n",
    "        reward_lst.append(reward)\n",
    "        #\n",
    "\n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, scores, 'b')\n",
    "            #pylab.savefig(\"./save_graph/aa_gun_dqn.png\")\n",
    "\n",
    "            print(\"episode:\", e, \"  score:\", score,np.mean(scores), \"  epsilon:\", agent.epsilon, 't:', t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14288"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
